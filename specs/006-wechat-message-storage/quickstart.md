# å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼šå¾®ä¿¡æ¶ˆæ¯æ•°æ®æ¹–å­˜å‚¨

**åŠŸèƒ½åˆ†æ”¯**: `006-wechat-message-storage`
**å‰ç½®æ¡ä»¶**: Python 3.12.6, è™šæ‹Ÿç¯å¢ƒå·²é…ç½®
**ç›¸å…³æ–‡æ¡£**: [spec.md](./spec.md), [plan.md](./plan.md), [data-model.md](./data-model.md)

## æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©å¼€å‘è€…å¿«é€Ÿä¸Šæ‰‹å¾®ä¿¡æ¶ˆæ¯æ•°æ®æ¹–å­˜å‚¨ç³»ç»Ÿ,æ¶µç›–ç¯å¢ƒé…ç½®ã€åŸºæœ¬ç”¨æ³•å’Œå¸¸è§åœºæ™¯ã€‚

---

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 å®‰è£…ä¾èµ–

```bash
# æ·»åŠ  Parquet å’Œæ–‡ä»¶é”ä¾èµ–
uv add pyarrow portalocker

# åŒæ­¥æ‰€æœ‰ä¾èµ–
uv sync --frozen
```

### 1.2 éªŒè¯å®‰è£…

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ(uv ä¼šè‡ªåŠ¨ç®¡ç†)
uv run python -c "import pyarrow; print(pyarrow.__version__)"
# é¢„æœŸè¾“å‡º: 14.x.x

uv run python -c "import portalocker; print('OK')"
# é¢„æœŸè¾“å‡º: OK
```

### 1.3 ç›®å½•ç»“æ„

ç¡®ä¿ä»¥ä¸‹ç›®å½•å­˜åœ¨:

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/messages/raw
mkdir -p data/parquet/messages
mkdir -p data/metadata/checkpoints
mkdir -p data/archive/messages
```

---

## 2. åŸºæœ¬ç”¨æ³•

### 2.1 è¿½åŠ æ¶ˆæ¯åˆ° JSONL

```python
from diting.services.storage.jsonl_writer import JSONLWriter

# åˆå§‹åŒ–å†™å…¥å™¨
writer = JSONLWriter(base_dir="data/messages/raw")

# è¿½åŠ å•æ¡æ¶ˆæ¯
message = {
    "msg_id": "1234567890",
    "from_username": "wxid_abc123",
    "to_username": "filehelper",
    "content": "Hello World",
    "create_time": 1737590400,
    "msg_type": 1,
    "is_chatroom_msg": 0,
    "chatroom": "",
    "chatroom_sender": "",
    "desc": "",
    "source": "0",
    "guid": "550e8400-e29b-41d4-a716-446655440000",
    "notify_type": 100
}

writer.append_message(message)
print("æ¶ˆæ¯å·²è¿½åŠ åˆ°å½“æ—¥ JSONL æ–‡ä»¶")
```

### 2.2 è½¬æ¢ JSONL åˆ° Parquet

```python
from diting.services.storage.jsonl_to_parquet import JSONLToParquetConverter
from pathlib import Path

# åˆå§‹åŒ–è½¬æ¢å™¨
converter = JSONLToParquetConverter(batch_size=10_000)

# è½¬æ¢å•ä¸ªæ–‡ä»¶
stats = converter.convert_to_parquet(
    jsonl_path=Path("data/messages/raw/2026-01-23.jsonl"),
    parquet_path=Path("data/parquet/messages/year=2026/month=01/day=23/data.parquet")
)

print(f"è½¬æ¢å®Œæˆ:")
print(f"  è®°å½•æ•°: {stats['total_records']}")
print(f"  å‹ç¼©ç‡: {stats['compression_ratio']:.2f}x")
print(f"  åŸå§‹å¤§å°: {stats['source_size_mb']:.2f} MB")
print(f"  å‹ç¼©å: {stats['target_size_mb']:.2f} MB")
```

### 2.3 æŸ¥è¯¢æ¶ˆæ¯

```python
from diting.services.storage.query import query_messages

# æŸ¥è¯¢æœ€è¿‘ 3 å¤©çš„æ‰€æœ‰æ¶ˆæ¯
df = query_messages(
    start_date="2026-01-20",
    end_date="2026-01-23",
    parquet_root="data/parquet/messages"
)

print(f"æŸ¥è¯¢åˆ° {len(df)} æ¡æ¶ˆæ¯")
print(df.head())

# æŸ¥è¯¢ç‰¹å®šç¾¤èŠçš„æ¶ˆæ¯
df_chatroom = query_messages(
    start_date="2026-01-20",
    end_date="2026-01-23",
    filters={"chatroom": "chatroom_123"},
    columns=["msg_id", "from_username", "content", "create_time"]
)

print(f"ç¾¤èŠæ¶ˆæ¯: {len(df_chatroom)} æ¡")
```

---

## 3. CLI å‘½ä»¤ç”¨æ³•

### 3.1 æ¯æ—¥æ‘„å…¥ä»»åŠ¡

```bash
# æ‘„å…¥ä»Šå¤©çš„ JSONL æ–‡ä»¶åˆ° Parquet
uv run python -m diting.cli.storage dump-parquet

# æ‘„å…¥æŒ‡å®šæ—¥æœŸ
uv run python -m diting.cli.storage dump-parquet --date 2026-01-23

# è·³è¿‡å·²å­˜åœ¨çš„ Parquet æ–‡ä»¶(é»˜è®¤)
uv run python -m diting.cli.storage dump-parquet --skip-existing

# å¼ºåˆ¶è¦†ç›–
uv run python -m diting.cli.storage dump-parquet --overwrite
```

### 3.2 æŸ¥è¯¢å‘½ä»¤

```bash
# æŸ¥è¯¢æœ€è¿‘ 7 å¤©çš„æ¶ˆæ¯
uv run python -m diting.cli.storage query \
    --start 2026-01-16 \
    --end 2026-01-23

# æŒ‰ç¾¤èŠè¿‡æ»¤
uv run python -m diting.cli.storage query \
    --start 2026-01-20 \
    --end 2026-01-23 \
    --chatroom chatroom_123

# å¯¼å‡ºä¸º CSV
uv run python -m diting.cli.storage query \
    --start 2026-01-20 \
    --end 2026-01-23 \
    --output messages.csv
```

### 3.3 éªŒè¯å‘½ä»¤

```bash
# éªŒè¯ç‰¹å®šåˆ†åŒº
uv run python -m diting.cli.storage validate \
    --partition "data/parquet/messages/year=2026/month=01/day=23"

# éªŒè¯æ‰€æœ‰åˆ†åŒº
uv run python -m diting.cli.storage validate --all

# æ£€æµ‹é‡å¤æ¶ˆæ¯
uv run python -m diting.cli.storage detect-duplicates
```

### 3.4 æ¸…ç†å‘½ä»¤

```bash
# æ¸…ç† 7 å¤©å‰çš„ JSONL æ–‡ä»¶(è¯•è¿è¡Œ)
uv run python -m diting.cli.storage cleanup \
    --retention-days 7 \
    --dry-run

# å®é™…æ‰§è¡Œæ¸…ç†
uv run python -m diting.cli.storage cleanup \
    --retention-days 7
```

---

## 4. è‡ªåŠ¨åŒ–ä»»åŠ¡é…ç½®

### 4.1 é…ç½® Systemd Timer

#### å®‰è£…æœåŠ¡æ–‡ä»¶

```bash
# å¤åˆ¶æœåŠ¡æ–‡ä»¶
sudo cp deploy/diting-parquet-dump.service /etc/systemd/system/
sudo cp deploy/diting-parquet-dump.timer /etc/systemd/system/

# é‡æ–°åŠ è½½ systemd
sudo systemctl daemon-reload

# å¯ç”¨ Timer
sudo systemctl enable diting-parquet-dump.timer

# å¯åŠ¨ Timer
sudo systemctl start diting-parquet-dump.timer
```

#### éªŒè¯ Timer

```bash
# æŸ¥çœ‹ Timer çŠ¶æ€
sudo systemctl status diting-parquet-dump.timer

# æŸ¥çœ‹ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
sudo systemctl list-timers diting-parquet-dump.timer

# æ‰‹åŠ¨è§¦å‘(æµ‹è¯•)
sudo systemctl start diting-parquet-dump.service

# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u diting-parquet-dump.service -f
```

#### ä¿®æ”¹æ‰§è¡Œæ—¶é—´

ç¼–è¾‘ `/etc/systemd/system/diting-parquet-dump.timer`:

```ini
[Timer]
# ä¿®æ”¹ä¸ºæ¯å¤©å‡Œæ™¨ 03:00 æ‰§è¡Œ
OnCalendar=*-*-* 03:00:00

# ä¿å­˜åé‡æ–°åŠ è½½
sudo systemctl daemon-reload
sudo systemctl restart diting-parquet-dump.timer
```

---

## 5. å¸¸è§åœºæ™¯

### 5.1 æ‰¹é‡å¯¼å…¥å†å²æ•°æ®

```python
from diting.services.storage.batch_converter import BatchConverter
from diting.services.storage.jsonl_to_parquet import JSONLToParquetConverter
from pathlib import Path

# åˆå§‹åŒ–æ‰¹é‡è½¬æ¢å™¨
converter = JSONLToParquetConverter(batch_size=10_000)
batch_converter = BatchConverter(
    raw_dir=Path("data/messages/raw"),
    parquet_dir=Path("data/parquet/messages"),
    converter=converter
)

# è½¬æ¢æ‰€æœ‰ JSONL æ–‡ä»¶
stats = batch_converter.convert_all(skip_existing=True)

print(f"æ‰¹é‡è½¬æ¢å®Œæˆ:")
print(f"  è½¬æ¢æ–‡ä»¶: {stats['converted']}")
print(f"  è·³è¿‡æ–‡ä»¶: {stats['skipped']}")
print(f"  å¤±è´¥æ–‡ä»¶: {stats['failed']}")
```

### 5.2 å¢é‡å¤„ç†æ–°æ¶ˆæ¯

```python
from diting.services.storage.incremental import incremental_ingest

# å¢é‡æ‘„å…¥(è‡ªåŠ¨ä»æ£€æŸ¥ç‚¹ç»§ç»­)
result = incremental_ingest(
    source_jsonl="data/messages/raw/2026-01-23.jsonl",
    parquet_root="data/parquet/messages",
    checkpoint_dir="data/metadata/checkpoints"
)

print(f"å¢é‡å¤„ç†å®Œæˆ:")
print(f"  æ–°å¢è®°å½•: {result['new_records']}")
print(f"  å»é‡å: {result['deduplicated']}")
print(f"  è·³è¿‡é‡å¤: {result['skipped_duplicates']}")
```

### 5.3 å½’æ¡£å†·æ•°æ®

```python
from diting.services.storage.archive import archive_old_partitions

# å½’æ¡£ 90 å¤©å‰çš„æ•°æ®
result = archive_old_partitions(
    parquet_root="data/parquet/messages",
    archive_root="data/archive/messages",
    older_than_days=90,
    compression="zstd",
    compression_level=19
)

print(f"å½’æ¡£å®Œæˆ:")
print(f"  å½’æ¡£åˆ†åŒº: {result['archived_partitions']}")
print(f"  åŸå§‹å¤§å°: {result['total_size_before_mb']:.2f} MB")
print(f"  å‹ç¼©å: {result['total_size_after_mb']:.2f} MB")
print(f"  å‹ç¼©ç‡: {result['compression_ratio']:.2f}x")
```

### 5.4 æŒ‰æ¡ä»¶è¿‡æ»¤æŸ¥è¯¢

```python
from diting.services.storage.query import query_messages

# æŸ¥è¯¢ç‰¹å®šç”¨æˆ·çš„æ¶ˆæ¯
df = query_messages(
    start_date="2026-01-01",
    end_date="2026-01-31",
    filters={
        "from_username": "wxid_abc123",
        "msg_type": 1  # æ–‡æœ¬æ¶ˆæ¯
    },
    columns=["msg_id", "content", "create_time"]
)

# æŒ‰æ—¶é—´æ’åº
df = df.sort_values("create_time")

# å¯¼å‡ºä¸º CSV
df.to_csv("user_messages.csv", index=False)
print(f"å·²å¯¼å‡º {len(df)} æ¡æ¶ˆæ¯åˆ° user_messages.csv")
```

### 5.5 éªŒè¯æ•°æ®å®Œæ•´æ€§

```python
from diting.services.storage.validation import validate_partition, detect_duplicates

# éªŒè¯ç‰¹å®šåˆ†åŒº
result = validate_partition("data/parquet/messages/year=2026/month=01/day=23")

if result["is_valid"]:
    print(f"âœ“ åˆ†åŒºæœ‰æ•ˆ:")
    print(f"  æ–‡ä»¶æ•°: {result['file_count']}")
    print(f"  è®°å½•æ•°: {result['total_records']}")
    print(f"  æ€»å¤§å°: {result['total_size_bytes'] / 1024 / 1024:.2f} MB")
else:
    print("âœ— åˆ†åŒºæ— æ•ˆ:")
    for error in result["errors"]:
        print(f"  - {error}")

# æ£€æµ‹é‡å¤æ¶ˆæ¯
duplicates = detect_duplicates("data/parquet/messages")

if len(duplicates) > 0:
    print(f"âš  å‘ç° {len(duplicates)} ä¸ªé‡å¤æ¶ˆæ¯:")
    print(duplicates)
else:
    print("âœ“ æ— é‡å¤æ¶ˆæ¯")
```

---

## 6. ç›‘æ§ä¸è°ƒè¯•

### 6.1 æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹ Systemd æœåŠ¡æ—¥å¿—
sudo journalctl -u diting-parquet-dump.service -n 100

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u diting-parquet-dump.service -f

# æŸ¥çœ‹ç‰¹å®šæ—¥æœŸçš„æ—¥å¿—
sudo journalctl -u diting-parquet-dump.service --since "2026-01-23 02:00:00"

# è¿‡æ»¤é”™è¯¯æ—¥å¿—
sudo journalctl -u diting-parquet-dump.service -p err
```

### 6.2 æ€§èƒ½åˆ†æ

```python
import time
from diting.services.storage.query import query_messages

# æµ‹é‡æŸ¥è¯¢æ€§èƒ½
start = time.time()
df = query_messages(
    start_date="2026-01-01",
    end_date="2026-01-31"
)
elapsed = time.time() - start

print(f"æŸ¥è¯¢ {len(df)} æ¡è®°å½•è€—æ—¶: {elapsed:.2f} ç§’")

# é¢„æœŸ: å•æ—¥æŸ¥è¯¢ <1 ç§’, æœˆåº¦æŸ¥è¯¢ <5 ç§’
```

### 6.3 ç£ç›˜ç©ºé—´ç›‘æ§

```bash
# æŸ¥çœ‹æ•°æ®ç›®å½•å¤§å°
du -sh data/messages/raw
du -sh data/parquet/messages
du -sh data/archive/messages

# æŸ¥çœ‹åˆ†åŒºè¯¦ç»†å¤§å°
du -h data/parquet/messages/year=2026/month=01/ | sort -h

# ç›‘æ§ç£ç›˜ä½¿ç”¨ç‡
df -h /opt/diting/data
```

---

## 7. æ•…éšœæ’æŸ¥

### 7.1 æ–‡ä»¶é”è¶…æ—¶

**é—®é¢˜**: `OSError: Failed to acquire file lock`

**åŸå› **: å¤šä¸ªè¿›ç¨‹åŒæ—¶å†™å…¥ JSONL æ–‡ä»¶

**è§£å†³**:
1. æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªå®šæ—¶ä»»åŠ¡åŒæ—¶è¿è¡Œ
2. å¢åŠ é”è¶…æ—¶æ—¶é—´(é»˜è®¤ 5 ç§’)
3. ç¡®ä¿ Systemd Timer é…ç½®æ­£ç¡®

```python
# å¢åŠ è¶…æ—¶æ—¶é—´
writer = JSONLWriter(base_dir="data/messages/raw", lock_timeout=10)
```

---

### 7.2 Parquet æ–‡ä»¶æŸå

**é—®é¢˜**: `pyarrow.lib.ArrowInvalid: Parquet file size is 0 bytes`

**åŸå› **: å†™å…¥è¿‡ç¨‹ä¸­è¿›ç¨‹å´©æºƒ

**è§£å†³**:
1. åˆ é™¤æŸåçš„ Parquet æ–‡ä»¶
2. é‡æ–°è¿è¡Œè½¬æ¢å‘½ä»¤

```bash
# æŸ¥æ‰¾ 0 å­—èŠ‚æ–‡ä»¶
find data/parquet -name "*.parquet" -size 0

# åˆ é™¤æŸåæ–‡ä»¶
find data/parquet -name "*.parquet" -size 0 -delete

# é‡æ–°è½¬æ¢
uv run python -m diting.cli.storage dump-parquet --date 2026-01-23
```

---

### 7.3 å†…å­˜ä¸è¶³

**é—®é¢˜**: `MemoryError` æˆ–è¿›ç¨‹è¢« OOM Killer ç»ˆæ­¢

**åŸå› **: æ‰¹é‡å¤§å°è¿‡å¤§æˆ–æ•°æ®é‡è¶…å‡ºé¢„æœŸ

**è§£å†³**:
1. å‡å°æ‰¹é‡å¤§å°(é»˜è®¤ 10,000 â†’ 5,000)
2. å¢åŠ ç³»ç»Ÿå†…å­˜é™åˆ¶

```python
# å‡å°æ‰¹é‡å¤§å°
converter = JSONLToParquetConverter(batch_size=5_000)
```

æˆ–ä¿®æ”¹ Systemd æœåŠ¡:

```ini
# /etc/systemd/system/diting-parquet-dump.service
[Service]
MemoryMax=4G  # å¢åŠ åˆ° 4GB
```

---

### 7.4 Schema ä¸å…¼å®¹

**é—®é¢˜**: `pyarrow.lib.ArrowInvalid: Schema mismatch`

**åŸå› **: å¾®ä¿¡ API è¿”å›æ•°æ®ç»“æ„å˜åŒ–

**è§£å†³**:
1. æŸ¥çœ‹é”™è¯¯æ—¥å¿—ç¡®å®šä¸å…¼å®¹å­—æ®µ
2. æ›´æ–° Schema å®šä¹‰
3. ä½¿ç”¨ `extra_fields` å­—æ®µä¿å­˜æœªçŸ¥å­—æ®µ

```python
# ä¸´æ—¶è§£å†³: è·³è¿‡ Schema éªŒè¯
converter.convert_to_parquet(
    jsonl_path=jsonl_path,
    parquet_path=parquet_path,
    schema=None  # è‡ªåŠ¨æ¨æ–­ Schema
)
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 æ•°æ®å¤‡ä»½

å®šæœŸå¤‡ä»½å…³é”®æ•°æ®:

```bash
# å¤‡ä»½ JSONL æ–‡ä»¶(æœ€è¿‘ 7 å¤©)
rsync -av --relative data/messages/raw ./backup/

# å¤‡ä»½ Parquet æ–‡ä»¶(æœ€è¿‘ 30 å¤©)
find data/parquet -mtime -30 -type f | rsync -av --files-from=- . ./backup/

# å¤‡ä»½æ£€æŸ¥ç‚¹
rsync -av data/metadata/checkpoints ./backup/
```

### 8.2 å®šæœŸç»´æŠ¤

æ¯æœˆæ‰§è¡Œç»´æŠ¤ä»»åŠ¡:

```bash
# 1. éªŒè¯æ‰€æœ‰åˆ†åŒº
uv run python -m diting.cli.storage validate --all

# 2. æ£€æµ‹é‡å¤æ¶ˆæ¯
uv run python -m diting.cli.storage detect-duplicates

# 3. å½’æ¡£å†·æ•°æ®
uv run python -m diting.cli.storage archive --older-than-days 90

# 4. æ¸…ç†è¿‡æœŸ JSONL
uv run python -m diting.cli.storage cleanup --retention-days 7
```

### 8.3 æ€§èƒ½ä¼˜åŒ–

- **æŸ¥è¯¢ä¼˜åŒ–**: å°½é‡ä½¿ç”¨æ—¥æœŸèŒƒå›´è¿‡æ»¤,é¿å…å…¨è¡¨æ‰«æ
- **åˆ—è£å‰ª**: åªæŸ¥è¯¢éœ€è¦çš„åˆ—,å‡å°‘ I/O
- **æ‰¹é‡å¤„ç†**: åˆå¹¶å¤šä¸ªå°æŸ¥è¯¢ä¸ºå•ä¸ªå¤§æŸ¥è¯¢
- **åˆ†åŒºåˆå¹¶**: å®šæœŸåˆå¹¶ <10MB çš„å°åˆ†åŒºæ–‡ä»¶

---

## 9. ä¸‹ä¸€æ­¥

- ğŸ“– é˜…è¯» [data-model.md](./data-model.md) äº†è§£æ•°æ®æ¨¡å‹è¯¦æƒ…
- ğŸ“– é˜…è¯» [contracts/storage-api.md](./contracts/storage-api.md) äº†è§£å®Œæ•´ API
- ğŸ”¨ æŸ¥çœ‹ [tasks.md](./tasks.md) äº†è§£å®ç°ä»»åŠ¡æ¸…å•
- ğŸ§ª è¿è¡Œæµ‹è¯•: `uv run pytest tests/unit/test_storage.py -v`

---

## 10. è·å–å¸®åŠ©

- **CLI å¸®åŠ©**: `uv run python -m diting.cli.storage --help`
- **API æ–‡æ¡£**: æŸ¥çœ‹ `contracts/storage-api.md`
- **é—®é¢˜åé¦ˆ**: åœ¨ GitHub Issues æäº¤é—®é¢˜
- **ä»£ç ç¤ºä¾‹**: æŸ¥çœ‹ `tests/integration/test_storage_pipeline.py`
