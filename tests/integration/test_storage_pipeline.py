"""å­˜å‚¨ç®¡é“é›†æˆæµ‹è¯•

æµ‹è¯•å®Œæ•´çš„å­˜å‚¨ç®¡é“ï¼šJSONL å†™å…¥ -> Parquet è½¬æ¢ -> æ•°æ®éªŒè¯
"""

import json
from pathlib import Path

import pyarrow.parquet as pq
import pytest
from diting.services.storage.ingestion import convert_jsonl_to_parquet
from diting.services.storage.jsonl_writer import JSONLWriter


class TestStoragePipelineIntegration:
    """æµ‹è¯•å®Œæ•´çš„å­˜å‚¨ç®¡é“"""

    @pytest.fixture
    def storage_dirs(self, tmp_path: Path) -> dict[str, Path]:
        """åˆ›å»ºå­˜å‚¨ç›®å½•"""
        return {
            "jsonl": tmp_path / "messages" / "raw",
            "parquet": tmp_path / "messages" / "parquet",
        }

    @pytest.fixture
    def sample_messages(self) -> list[dict]:
        """åˆ›å»ºç¤ºä¾‹æ¶ˆæ¯"""
        base_timestamp = 1737590400  # 2025-01-23 00:00:00 UTC

        return [
            {
                "msg_id": f"msg_{i}",
                "from_username": f"wxid_sender_{i}",
                "to_username": f"wxid_receiver_{i}",
                "chatroom": "",
                "chatroom_sender": "",
                "msg_type": 1,
                "create_time": base_timestamp + i,
                "is_chatroom_msg": 0,
                "content": f"Message {i}",
                "desc": "",
                "source": "0",
                "guid": f"guid_{i}",
                "notify_type": 100,
            }
            for i in range(100)
        ]

    def test_end_to_end_storage_pipeline(
        self, storage_dirs: dict[str, Path], sample_messages: list[dict]
    ):
        """æµ‹è¯•ç«¯åˆ°ç«¯å­˜å‚¨ç®¡é“"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        # æ­¥éª¤ 1: å†™å…¥ JSONL
        writer = JSONLWriter(base_dir=jsonl_dir)
        writer.append_batch(sample_messages)

        # éªŒè¯ JSONL æ–‡ä»¶è¢«åˆ›å»º
        jsonl_file = writer._get_current_file_path()
        assert jsonl_file.exists()

        # æ­¥éª¤ 2: è½¬æ¢ä¸º Parquet
        result = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        # éªŒè¯è½¬æ¢ç»“æœ
        assert result["total_records"] == 100
        assert result["source_size_mb"] > 0
        assert result["target_size_mb"] > 0
        assert result["compression_ratio"] > 0

        # æ­¥éª¤ 3: éªŒè¯ Parquet æ•°æ®
        partition_dir = parquet_dir / "year=2025" / "month=01" / "day=23"
        parquet_file = partition_dir / "data.parquet"
        assert parquet_file.exists()

        # è¯»å–å¹¶éªŒè¯æ•°æ®
        table = pq.read_table(parquet_file)
        assert len(table) == 100

        # éªŒè¯å­—æ®µ
        assert "msg_id" in table.column_names
        assert "from_username" in table.column_names
        assert "create_time" in table.column_names
        assert "ingestion_time" in table.column_names

        # éªŒè¯æ•°æ®å†…å®¹
        df = table.to_pandas()
        assert df["msg_id"].iloc[0] == "msg_0"
        assert df["content"].iloc[0] == "Message 0"

    def test_incremental_storage_pipeline(
        self, storage_dirs: dict[str, Path], sample_messages: list[dict]
    ):
        """æµ‹è¯•å¢é‡å­˜å‚¨ç®¡é“"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        writer = JSONLWriter(base_dir=jsonl_dir)

        # ç¬¬ä¸€æ‰¹æ¶ˆæ¯
        batch1 = sample_messages[:50]
        writer.append_batch(batch1)

        jsonl_file = writer._get_current_file_path()
        result1 = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        assert result1["total_records"] == 50

        # ç¬¬äºŒæ‰¹æ¶ˆæ¯ï¼ˆè¿½åŠ åˆ°åŒä¸€ä¸ª JSONL æ–‡ä»¶ï¼‰
        batch2 = sample_messages[50:]
        writer.append_batch(batch2)

        # é‡æ–°è½¬æ¢ï¼ˆä¼šè¦†ç›–ä¹‹å‰çš„ Parquet æ–‡ä»¶ï¼‰
        result2 = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        assert result2["total_records"] == 100

        # éªŒè¯æœ€ç»ˆæ•°æ®
        partition_dir = parquet_dir / "year=2025" / "month=01" / "day=23"
        parquet_file = partition_dir / "data.parquet"

        table = pq.read_table(parquet_file)
        assert len(table) == 100

    def test_multi_day_storage_pipeline(self, storage_dirs: dict[str, Path]):
        """æµ‹è¯•å¤šå¤©æ•°æ®çš„å­˜å‚¨ç®¡é“"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        # åˆ›å»ºè·¨è¶Š 3 å¤©çš„æ¶ˆæ¯
        messages = []
        for day in range(3):
            base_timestamp = 1737590400 + (day * 86400)  # æ¯å¤© 86400 ç§’
            for i in range(10):
                messages.append(
                    {
                        "msg_id": f"msg_day{day}_{i}",
                        "from_username": f"wxid_{i}",
                        "to_username": "wxid_receiver",
                        "chatroom": "",
                        "chatroom_sender": "",
                        "msg_type": 1,
                        "create_time": base_timestamp + i,
                        "is_chatroom_msg": 0,
                        "content": f"Day {day} Message {i}",
                        "desc": "",
                        "source": "0",
                        "guid": f"guid_day{day}_{i}",
                        "notify_type": 100,
                    }
                )

        # å†™å…¥ JSONL
        writer = JSONLWriter(base_dir=jsonl_dir)
        writer.append_batch(messages)

        # è½¬æ¢ä¸º Parquet
        jsonl_file = writer._get_current_file_path()
        result = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        assert result["total_records"] == 30

        # éªŒè¯ 3 ä¸ªåˆ†åŒºéƒ½è¢«åˆ›å»º
        for day in range(3):
            day_num = 23 + day
            partition_dir = parquet_dir / "year=2025" / "month=01" / f"day={day_num:02d}"
            parquet_file = partition_dir / "data.parquet"
            assert parquet_file.exists()

            # éªŒè¯æ¯ä¸ªåˆ†åŒºæœ‰ 10 æ¡è®°å½•
            table = pq.read_table(parquet_file)
            assert len(table) == 10

    def test_storage_pipeline_with_invalid_messages(
        self, storage_dirs: dict[str, Path], sample_messages: list[dict]
    ):
        """æµ‹è¯•åŒ…å«æ— æ•ˆæ¶ˆæ¯çš„å­˜å‚¨ç®¡é“"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        # æ·»åŠ ä¸€äº›æ— æ•ˆæ¶ˆæ¯
        invalid_messages = [
            {"msg_id": "invalid_1"},  # ç¼ºå°‘å¿…å¡«å­—æ®µ
            {"msg_id": "invalid_2", "content": "No timestamp"},
        ]

        all_messages = sample_messages + invalid_messages

        # å†™å…¥ JSONLï¼ˆåŒ…æ‹¬æ— æ•ˆæ¶ˆæ¯ï¼‰
        writer = JSONLWriter(base_dir=jsonl_dir)
        writer.append_batch(all_messages)

        # è½¬æ¢ä¸º Parquetï¼ˆåº”è¯¥è¿‡æ»¤æ‰æ— æ•ˆæ¶ˆæ¯ï¼‰
        jsonl_file = writer._get_current_file_path()
        result = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        # åªæœ‰æœ‰æ•ˆæ¶ˆæ¯è¢«è½¬æ¢
        assert result["total_records"] == 100

    def test_storage_pipeline_compression_ratio(
        self, storage_dirs: dict[str, Path], sample_messages: list[dict]
    ):
        """æµ‹è¯•å­˜å‚¨ç®¡é“çš„å‹ç¼©æ¯”"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        writer = JSONLWriter(base_dir=jsonl_dir)
        writer.append_batch(sample_messages)

        jsonl_file = writer._get_current_file_path()
        result = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        # éªŒè¯å‹ç¼©æ¯” > 1ï¼ˆParquet åº”è¯¥æ¯” JSONL å°ï¼‰
        assert result["compression_ratio"] > 1.0

        # éªŒè¯æ–‡ä»¶å¤§å°
        assert result["source_size_mb"] > result["target_size_mb"]

    def test_storage_pipeline_preserves_data_integrity(
        self, storage_dirs: dict[str, Path], sample_messages: list[dict]
    ):
        """æµ‹è¯•å­˜å‚¨ç®¡é“ä¿æŒæ•°æ®å®Œæ•´æ€§"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        writer = JSONLWriter(base_dir=jsonl_dir)
        writer.append_batch(sample_messages)

        jsonl_file = writer._get_current_file_path()
        convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        # è¯»å– Parquet æ•°æ®
        partition_dir = parquet_dir / "year=2025" / "month=01" / "day=23"
        parquet_file = partition_dir / "data.parquet"
        table = pq.read_table(parquet_file)
        df = table.to_pandas()

        # éªŒè¯æ‰€æœ‰åŸå§‹æ¶ˆæ¯çš„æ•°æ®éƒ½å­˜åœ¨
        for _i, original_msg in enumerate(sample_messages):
            row = df[df["msg_id"] == original_msg["msg_id"]].iloc[0]

            assert row["from_username"] == original_msg["from_username"]
            assert row["to_username"] == original_msg["to_username"]
            assert row["content"] == original_msg["content"]
            assert row["msg_type"] == original_msg["msg_type"]

    def test_storage_pipeline_with_unicode_content(self, storage_dirs: dict[str, Path]):
        """æµ‹è¯•åŒ…å« Unicode å†…å®¹çš„å­˜å‚¨ç®¡é“"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        messages = [
            {
                "msg_id": "msg_unicode",
                "from_username": "wxid_sender",
                "to_username": "wxid_receiver",
                "chatroom": "",
                "chatroom_sender": "",
                "msg_type": 1,
                "create_time": 1737590400,
                "is_chatroom_msg": 0,
                "content": "ä½ å¥½ä¸–ç•Œ ğŸŒ Hello World",
                "desc": "æµ‹è¯•æè¿°",
                "source": "0",
                "guid": "guid_unicode",
                "notify_type": 100,
            }
        ]

        writer = JSONLWriter(base_dir=jsonl_dir)
        writer.append_batch(messages)

        jsonl_file = writer._get_current_file_path()
        convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        # éªŒè¯ Unicode å†…å®¹è¢«æ­£ç¡®ä¿å­˜
        partition_dir = parquet_dir / "year=2025" / "month=01" / "day=23"
        parquet_file = partition_dir / "data.parquet"
        table = pq.read_table(parquet_file)
        df = table.to_pandas()

        assert df["content"].iloc[0] == "ä½ å¥½ä¸–ç•Œ ğŸŒ Hello World"
        assert df["desc"].iloc[0] == "æµ‹è¯•æè¿°"


class TestStoragePipelineErrorHandling:
    """æµ‹è¯•å­˜å‚¨ç®¡é“çš„é”™è¯¯å¤„ç†"""

    @pytest.fixture
    def storage_dirs(self, tmp_path: Path) -> dict[str, Path]:
        """åˆ›å»ºå­˜å‚¨ç›®å½•"""
        return {
            "jsonl": tmp_path / "messages" / "raw",
            "parquet": tmp_path / "messages" / "parquet",
        }

    def test_convert_nonexistent_jsonl_file(self, storage_dirs: dict[str, Path]):
        """æµ‹è¯•è½¬æ¢ä¸å­˜åœ¨çš„ JSONL æ–‡ä»¶"""
        jsonl_file = storage_dirs["jsonl"] / "nonexistent.jsonl"
        parquet_dir = storage_dirs["parquet"]

        with pytest.raises(FileNotFoundError):
            convert_jsonl_to_parquet(jsonl_file, parquet_dir)

    def test_convert_empty_jsonl_file(self, storage_dirs: dict[str, Path]):
        """æµ‹è¯•è½¬æ¢ç©º JSONL æ–‡ä»¶"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        # åˆ›å»ºç©º JSONL æ–‡ä»¶
        jsonl_dir.mkdir(parents=True, exist_ok=True)
        jsonl_file = jsonl_dir / "empty.jsonl"
        jsonl_file.touch()

        result = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        assert result["total_records"] == 0

    def test_convert_malformed_jsonl_file(self, storage_dirs: dict[str, Path]):
        """æµ‹è¯•è½¬æ¢æ ¼å¼é”™è¯¯çš„ JSONL æ–‡ä»¶"""
        jsonl_dir = storage_dirs["jsonl"]
        parquet_dir = storage_dirs["parquet"]

        # åˆ›å»ºåŒ…å«æ— æ•ˆ JSON çš„æ–‡ä»¶
        jsonl_dir.mkdir(parents=True, exist_ok=True)
        jsonl_file = jsonl_dir / "malformed.jsonl"

        with open(jsonl_file, "w", encoding="utf-8") as f:
            f.write('{"msg_id": "valid"}\n')
            f.write("invalid json line\n")  # æ— æ•ˆè¡Œ
            f.write('{"msg_id": "valid2"}\n')

        # åº”è¯¥è·³è¿‡æ— æ•ˆè¡Œå¹¶ç»§ç»­å¤„ç†
        result = convert_jsonl_to_parquet(jsonl_file, parquet_dir)

        # åªæœ‰æœ‰æ•ˆçš„è¡Œè¢«å¤„ç†ï¼ˆä½†å¯èƒ½å› ä¸ºç¼ºå°‘å¿…å¡«å­—æ®µè€Œè¢«è¿‡æ»¤ï¼‰
        # è¿™é‡Œä¸»è¦éªŒè¯ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        assert result is not None


class TestStoragePipelineConcurrency:
    """æµ‹è¯•å­˜å‚¨ç®¡é“çš„å¹¶å‘æ€§"""

    @pytest.fixture
    def storage_dirs(self, tmp_path: Path) -> dict[str, Path]:
        """åˆ›å»ºå­˜å‚¨ç›®å½•"""
        return {
            "jsonl": tmp_path / "messages" / "raw",
            "parquet": tmp_path / "messages" / "parquet",
        }

    def test_concurrent_jsonl_writes(self, storage_dirs: dict[str, Path]):
        """æµ‹è¯•å¹¶å‘ JSONL å†™å…¥"""
        import threading

        jsonl_dir = storage_dirs["jsonl"]
        writer = JSONLWriter(base_dir=jsonl_dir)

        def write_batch(start_id: int, count: int):
            messages = [
                {
                    "msg_id": f"msg_{start_id + i}",
                    "from_username": f"wxid_{i}",
                    "to_username": "wxid_receiver",
                    "chatroom": "",
                    "chatroom_sender": "",
                    "msg_type": 1,
                    "create_time": 1737590400 + i,
                    "is_chatroom_msg": 0,
                    "content": f"Message {i}",
                    "desc": "",
                    "source": "0",
                    "guid": f"guid_{i}",
                    "notify_type": 100,
                }
                for i in range(count)
            ]
            writer.append_batch(messages)

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹å¹¶å‘å†™å…¥
        threads = []
        for i in range(5):
            thread = threading.Thread(target=write_batch, args=(i * 20, 20))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # éªŒè¯æ‰€æœ‰æ¶ˆæ¯éƒ½è¢«å†™å…¥
        jsonl_file = writer._get_current_file_path()
        with open(jsonl_file, encoding="utf-8") as f:
            lines = f.readlines()

        assert len(lines) == 100

        # éªŒè¯æ‰€æœ‰æ¶ˆæ¯ ID å”¯ä¸€
        msg_ids = set()
        for line in lines:
            msg = json.loads(line)
            msg_ids.add(msg["msg_id"])

        assert len(msg_ids) == 100
